{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc3a4200810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import necessary libs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from vanilla_resnet import DACNN_RES,SmallBlock\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "valid_size = 0.2\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    \n",
    "])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = torchvision.datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = torchvision.datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=test_transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DACNN_RES(\n",
       "  (conv_1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_global): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (sections): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (8): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (2): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): SmallBlock(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (clf): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DACNN_RES()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader,model):\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    for data,labels in loader:\n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        output = model(data)\n",
    "        _,pred = torch.max(output,1) ## get the predictions\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        correct  += sum(np.squeeze(correct_tensor.cpu().numpy()))\n",
    "    return correct/len(loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_by_class(loader,model):\n",
    "    ## calculate per-class accuracy\n",
    "    correct = [0]*10\n",
    "    total = [0]*10\n",
    "    model.eval()\n",
    "    for data,labels in loader:\n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        output = model(data)\n",
    "        _,pred = torch.max(output,1) ## get the predictions\n",
    "        correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "        \n",
    "        correct_np = correct_tensor.cpu().numpy().squeeze()\n",
    "        labels = labels.cpu().numpy().squeeze()\n",
    "        \n",
    "        for pred,label in zip(correct_np,labels):\n",
    "            correct[label] += pred\n",
    "            total[label] += 1 ## count total no of examples\n",
    "    \n",
    "    return correct,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_list(loader,model):\n",
    "    ## calculate per-class accuracy\n",
    "    pred_list = []\n",
    "    true_list = []\n",
    "    model.eval()\n",
    "    for data,labels in loader:\n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        output = model(data)\n",
    "        _,pred = torch.max(output,1) ## get the predictions\n",
    "        pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "        labels = labels.cpu().numpy().squeeze()\n",
    "        \n",
    "        pred_list.extend(pred_np)\n",
    "        true_list.extend(labels)\n",
    "        \n",
    "    return np.asarray(pred_list),np.asarray(true_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 1 \tTraining Loss : 1.6851723158836365 \tValidation Loss :1.3744869247436524 \tValidation Acc : 0.5093\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (-inf --> 0.509300).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 2 \tTraining Loss : 1.0602142126083374 \tValidation Loss :0.9638714912414551 \tValidation Acc : 0.6607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.509300 --> 0.660700).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 3 \tTraining Loss : 0.8828075497627258 \tValidation Loss :0.9240636863708496 \tValidation Acc : 0.6764\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.660700 --> 0.676400).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 4 \tTraining Loss : 0.768561780166626 \tValidation Loss :0.8822661415100098 \tValidation Acc : 0.6979\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.676400 --> 0.697900).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 5 \tTraining Loss : 0.6906439895153046 \tValidation Loss :0.7987429271697998 \tValidation Acc : 0.7228\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.697900 --> 0.722800).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 6 \tTraining Loss : 0.6350052696228027 \tValidation Loss :0.7406992198944092 \tValidation Acc : 0.7489\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.722800 --> 0.748900).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 7 \tTraining Loss : 0.5865327599048614 \tValidation Loss :0.6714816143035889 \tValidation Acc : 0.7733\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.748900 --> 0.773300).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 8 \tTraining Loss : 0.5487395999908448 \tValidation Loss :0.6652192600250244 \tValidation Acc : 0.7698\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 9 \tTraining Loss : 0.5146903744697571 \tValidation Loss :0.6948606384277344 \tValidation Acc : 0.7689\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 10 \tTraining Loss : 0.4857960311174393 \tValidation Loss :0.6191144353866577 \tValidation Acc : 0.7877\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.773300 --> 0.787700).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 11 \tTraining Loss : 0.4614293076515198 \tValidation Loss :0.6276293424606323 \tValidation Acc : 0.7899\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.787700 --> 0.789900).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 12 \tTraining Loss : 0.4374056991815567 \tValidation Loss :0.6190075006484985 \tValidation Acc : 0.7926\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.789900 --> 0.792600).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 13 \tTraining Loss : 0.41118661286830904 \tValidation Loss :0.6176837341308594 \tValidation Acc : 0.7985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.792600 --> 0.798500).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 14 \tTraining Loss : 0.39280316729545595 \tValidation Loss :0.6109208301544189 \tValidation Acc : 0.8021\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.798500 --> 0.802100).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 15 \tTraining Loss : 0.37125028779506686 \tValidation Loss :0.6181141948699951 \tValidation Acc : 0.7938\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 16 \tTraining Loss : 0.3564721843719482 \tValidation Loss :0.6084978073120118 \tValidation Acc : 0.8026\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.802100 --> 0.802600).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 17 \tTraining Loss : 0.334850200510025 \tValidation Loss :0.6211725129127502 \tValidation Acc : 0.8008\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 18 \tTraining Loss : 0.31867252743244173 \tValidation Loss :0.64531369972229 \tValidation Acc : 0.7994\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 19 \tTraining Loss : 0.3027599257707596 \tValidation Loss :0.6027096279144287 \tValidation Acc : 0.8071\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.802600 --> 0.807100).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 20 \tTraining Loss : 0.2923368209481239 \tValidation Loss :0.6188620901107789 \tValidation Acc : 0.8057\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 21 \tTraining Loss : 0.27460593734979627 \tValidation Loss :0.5997459342956543 \tValidation Acc : 0.8074\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.807100 --> 0.807400).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 22 \tTraining Loss : 0.2633904290676117 \tValidation Loss :0.6274113019943237 \tValidation Acc : 0.803\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 23 \tTraining Loss : 0.25038948724269866 \tValidation Loss :0.6412035785675049 \tValidation Acc : 0.808\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.807400 --> 0.808000).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 24 \tTraining Loss : 0.23999271981716155 \tValidation Loss :0.7015583058357239 \tValidation Acc : 0.7974\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 25 \tTraining Loss : 0.22726630411148072 \tValidation Loss :0.6589530183792114 \tValidation Acc : 0.8066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 26 \tTraining Loss : 0.21602710723876953 \tValidation Loss :0.6785626970291138 \tValidation Acc : 0.8071\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 27 \tTraining Loss : 0.20712984842061996 \tValidation Loss :0.6966114094734192 \tValidation Acc : 0.8043\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 28 \tTraining Loss : 0.19738884135484697 \tValidation Loss :0.6547501134872437 \tValidation Acc : 0.8091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.808000 --> 0.809100).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 29 \tTraining Loss : 0.18553658668994905 \tValidation Loss :0.6691217491149902 \tValidation Acc : 0.8116\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.809100 --> 0.811600).  Saving model ...\n",
      "Part 1 Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 Started\n",
      "Epoch : 30 \tTraining Loss : 0.17835342462062836 \tValidation Loss :0.6869901029586792 \tValidation Acc : 0.8119\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.811600 --> 0.811900).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 31 \tTraining Loss : 0.17014710170030595 \tValidation Loss :0.7141573665618897 \tValidation Acc : 0.8076\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 32 \tTraining Loss : 0.16121800878047943 \tValidation Loss :0.726026975440979 \tValidation Acc : 0.8076\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 33 \tTraining Loss : 0.1531729877948761 \tValidation Loss :0.7350792072296143 \tValidation Acc : 0.8116\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 34 \tTraining Loss : 0.14173251680135726 \tValidation Loss :0.7366155447006225 \tValidation Acc : 0.8119\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.811900 --> 0.811900).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 35 \tTraining Loss : 0.13970662922859192 \tValidation Loss :0.7575963878631592 \tValidation Acc : 0.807\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 36 \tTraining Loss : 0.13192047218084335 \tValidation Loss :0.7588088683128357 \tValidation Acc : 0.8087\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 37 \tTraining Loss : 0.12734649416208269 \tValidation Loss :0.7658732226371765 \tValidation Acc : 0.8048\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 38 \tTraining Loss : 0.11781436628103256 \tValidation Loss :0.804198530960083 \tValidation Acc : 0.8058\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 39 \tTraining Loss : 0.11733948123455047 \tValidation Loss :0.8214815477371216 \tValidation Acc : 0.8067\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 40 \tTraining Loss : 0.11177826064825058 \tValidation Loss :0.8110084981918335 \tValidation Acc : 0.8138\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.811900 --> 0.813800).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 41 \tTraining Loss : 0.10866144211292267 \tValidation Loss :0.8358471231460571 \tValidation Acc : 0.8059\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 42 \tTraining Loss : 0.09897490736246109 \tValidation Loss :0.8549681686401367 \tValidation Acc : 0.8043\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 43 \tTraining Loss : 0.09796816051006317 \tValidation Loss :0.8412547590255738 \tValidation Acc : 0.809\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 44 \tTraining Loss : 0.09066590046882629 \tValidation Loss :0.843673283290863 \tValidation Acc : 0.8111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 45 \tTraining Loss : 0.08596743588447571 \tValidation Loss :0.815179460144043 \tValidation Acc : 0.809\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 46 \tTraining Loss : 0.08481345444917679 \tValidation Loss :0.8819000173568725 \tValidation Acc : 0.8076\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 47 \tTraining Loss : 0.07824212629795074 \tValidation Loss :0.9079805738449097 \tValidation Acc : 0.8033\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 48 \tTraining Loss : 0.07588090388774872 \tValidation Loss :0.9150898027420044 \tValidation Acc : 0.8066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 49 \tTraining Loss : 0.07679430695772171 \tValidation Loss :0.9243465133666993 \tValidation Acc : 0.8155\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.813800 --> 0.815500).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 50 \tTraining Loss : 0.07403973447084426 \tValidation Loss :0.8931075320243835 \tValidation Acc : 0.8139\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 51 \tTraining Loss : 0.07232418922185897 \tValidation Loss :0.8998575538635254 \tValidation Acc : 0.8097\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 52 \tTraining Loss : 0.06575718179941177 \tValidation Loss :0.9255338342666626 \tValidation Acc : 0.8118\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 53 \tTraining Loss : 0.06576942412853241 \tValidation Loss :0.9261440099716186 \tValidation Acc : 0.8074\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 54 \tTraining Loss : 0.061730572402477264 \tValidation Loss :0.9508117372512818 \tValidation Acc : 0.8105\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 55 \tTraining Loss : 0.05565460211038589 \tValidation Loss :0.9748667512893677 \tValidation Acc : 0.8093\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 56 \tTraining Loss : 0.05823226538896561 \tValidation Loss :0.9587035425186157 \tValidation Acc : 0.8075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 57 \tTraining Loss : 0.05300946385264397 \tValidation Loss :0.9788414665222168 \tValidation Acc : 0.808\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 58 \tTraining Loss : 0.05373009749650955 \tValidation Loss :1.0351763946533203 \tValidation Acc : 0.8048\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 59 \tTraining Loss : 0.04755821733474731 \tValidation Loss :0.9752210256576538 \tValidation Acc : 0.8157\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Validation Acc. increased (0.815500 --> 0.815700).  Saving model ...\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 60 \tTraining Loss : 0.049292943978309633 \tValidation Loss :0.9929180152893067 \tValidation Acc : 0.8095\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 61 \tTraining Loss : 0.05013494279384613 \tValidation Loss :1.0596283962249755 \tValidation Acc : 0.8071\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 62 \tTraining Loss : 0.04943106781244278 \tValidation Loss :1.034473634338379 \tValidation Acc : 0.8106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 Started\n",
      "Epoch : 63 \tTraining Loss : 0.046679657363891604 \tValidation Loss :1.0452733249664308 \tValidation Acc : 0.8075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 64 \tTraining Loss : 0.041168723952770235 \tValidation Loss :1.0496207653045655 \tValidation Acc : 0.808\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 65 \tTraining Loss : 0.040719001698493955 \tValidation Loss :1.0285468217849731 \tValidation Acc : 0.8099\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 66 \tTraining Loss : 0.03809108040332794 \tValidation Loss :1.0631483386993408 \tValidation Acc : 0.806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 67 \tTraining Loss : 0.03883589534759521 \tValidation Loss :1.0981500638961792 \tValidation Acc : 0.8106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 68 \tTraining Loss : 0.039012364292144774 \tValidation Loss :1.0637347606658936 \tValidation Acc : 0.8102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 69 \tTraining Loss : 0.03617366012334824 \tValidation Loss :1.1135615953445435 \tValidation Acc : 0.8111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 70 \tTraining Loss : 0.03494731109142304 \tValidation Loss :1.0466108530044556 \tValidation Acc : 0.8129\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 71 \tTraining Loss : 0.03665510882139206 \tValidation Loss :1.1005524882316589 \tValidation Acc : 0.8061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 72 \tTraining Loss : 0.03494527431726456 \tValidation Loss :1.0926003816604615 \tValidation Acc : 0.8114\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 73 \tTraining Loss : 0.03310679486989975 \tValidation Loss :1.0916669412612916 \tValidation Acc : 0.8059\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 74 \tTraining Loss : 0.031226379752159118 \tValidation Loss :1.1029938938140869 \tValidation Acc : 0.8128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 75 \tTraining Loss : 0.030485292172431944 \tValidation Loss :1.116762017440796 \tValidation Acc : 0.8128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 76 \tTraining Loss : 0.030608976995944975 \tValidation Loss :1.0911645931243896 \tValidation Acc : 0.8147\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 77 \tTraining Loss : 0.029618711376190186 \tValidation Loss :1.1241622217178344 \tValidation Acc : 0.8065\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 78 \tTraining Loss : 0.030669345223903657 \tValidation Loss :1.1762202402114867 \tValidation Acc : 0.8098\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 79 \tTraining Loss : 0.031297303807735447 \tValidation Loss :1.1534919916152955 \tValidation Acc : 0.8089\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 80 \tTraining Loss : 0.027619060933589935 \tValidation Loss :1.1233352115631103 \tValidation Acc : 0.8145\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 81 \tTraining Loss : 0.02612498083114624 \tValidation Loss :1.122547200012207 \tValidation Acc : 0.8111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 82 \tTraining Loss : 0.02592095444202423 \tValidation Loss :1.1713432153701782 \tValidation Acc : 0.8137\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 83 \tTraining Loss : 0.026240782725811006 \tValidation Loss :1.1221865949630738 \tValidation Acc : 0.8086\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 84 \tTraining Loss : 0.02607359458208084 \tValidation Loss :1.1298246496200561 \tValidation Acc : 0.8128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 85 \tTraining Loss : 0.026721275806427 \tValidation Loss :1.1688262008666992 \tValidation Acc : 0.8112\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 86 \tTraining Loss : 0.025111044681072236 \tValidation Loss :1.1676027839660645 \tValidation Acc : 0.8111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 87 \tTraining Loss : 0.024157630670070648 \tValidation Loss :1.1949094064712524 \tValidation Acc : 0.8153\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 88 \tTraining Loss : 0.0233379665851593 \tValidation Loss :1.1943707921981812 \tValidation Acc : 0.8111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 89 \tTraining Loss : 0.02472405825853348 \tValidation Loss :1.1657538312911988 \tValidation Acc : 0.8119\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Part 1 Started\n",
      "Part 2 Started\n",
      "Epoch : 90 \tTraining Loss : 0.024022412192821502 \tValidation Loss :1.165566202545166 \tValidation Acc : 0.8141\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "n_epochs = 90\n",
    "valid_acc_max = -np.Inf\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(1,n_epochs+1): \n",
    "    \n",
    "    train_loss = 0.0 ## running losses\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train() ## training mode\n",
    "    print('Part 1 Started')\n",
    "    for data,labels in train_loader:\n",
    "        \n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        batch_size = data.size(0)\n",
    "        \n",
    "        optimizer.zero_grad() ## clear the gradient\n",
    "        output = model(data) ## get the output\n",
    "        loss = criterion(output,labels) ## get the output\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*batch_size\n",
    "    \n",
    "    model.eval()\n",
    "    print('Part 2 Started')\n",
    "    for data,labels in valid_loader:\n",
    "        \n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output,labels)\n",
    "        valid_loss += loss.item()*batch_size    \n",
    "    \n",
    "    \n",
    "    train_loss /= len(train_loader.sampler)\n",
    "    valid_loss /= len(valid_loader.sampler)\n",
    "    \n",
    "    \n",
    "    valid_acc = accuracy(valid_loader,model)\n",
    "    \n",
    "    valid_losses.append(valid_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    val_acc.append(valid_acc)\n",
    "    \n",
    "        \n",
    "    print('Epoch : {} \\tTraining Loss : {} \\tValidation Loss :{} \\tValidation Acc : {}'.format(epoch,train_loss,valid_loss,valid_acc))\n",
    "    print('-'*100)\n",
    "    if valid_acc >= valid_acc_max:\n",
    "        print('Validation Acc. increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_acc_max,\n",
    "        valid_acc))\n",
    "        torch.save(model.state_dict(), './vanilla_resnet.pt')\n",
    "        valid_acc_max = valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./vanilla_resnet.pt')) ## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[858, 906, 742, 666, 784, 689, 849, 851, 883, 920]\n",
      "[1024, 1014, 967, 1027, 972, 990, 960, 1036, 989, 1021]\n",
      "[0.837890625, 0.893491124260355, 0.7673216132368149, 0.6484907497565725, 0.8065843621399177, 0.695959595959596, 0.884375, 0.8214285714285714, 0.8928210313447927, 0.901077375122429]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8148"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct,total = acc_by_class(valid_loader,model)\n",
    "print(correct)\n",
    "print(total)\n",
    "acc  = [i/j for i,j in zip(correct,total)]\n",
    "print(acc)\n",
    "sum(correct)/sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[838, 901, 754, 649, 807, 752, 888, 824, 878, 890]\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "[0.838, 0.901, 0.754, 0.649, 0.807, 0.752, 0.888, 0.824, 0.878, 0.89]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8181"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct,total = acc_by_class(test_loader,model)\n",
    "print(correct)\n",
    "print(total)\n",
    "acc  = [i/j for i,j in zip(correct,total)]\n",
    "print(acc)\n",
    "sum(correct)/sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f6fccd76f2ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_by_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_loader' is not defined"
     ]
    }
   ],
   "source": [
    "correct,total = acc_by_class(_loader,model)\n",
    "print(correct)\n",
    "print(total)\n",
    "acc  = [i/j for i,j in zip(correct,total)]\n",
    "print(acc)\n",
    "sum(correct)/sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
